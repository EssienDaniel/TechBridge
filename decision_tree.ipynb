{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "* Belong to the family of supervised learning algorithms\n",
    "* Can be use to solve both regression and classification problem\n",
    "\n",
    "The goal of using a Decision Tree is to create a training model that can be use to predict the class or value of the target variable by learning simple decision rules inferred from prior data(training data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Decision Trees\n",
    "Types of Decision Tree is based on the type of target variable.\n",
    "\n",
    "1. Categorical Variable Decision\n",
    "2. Continuous Variable Decision\n",
    "\n",
    "## Important Terminolgy related to Decision Trees\n",
    "* Root Node: It represents the entire population or sample and this further gets divided into two or more homogeneous sets.\n",
    "* Splittng: It is a process of dividing a node into two or more sub-nodes\n",
    "* Decision Node: When a sub-node splits into further sub-nodes, then it is called the decision node.\n",
    "* Leaf/Terminal Node: Nodes that do not split is called Leaf or Terminal node.\n",
    "* Pruning: When we remove sub-nodes of a decision node, this process is called pruning. You can say the opposite process of splitting.\n",
    "* Branch/Sub-Tree: A subsection of the entire tree is called branch or subtree.\n",
    "* Parent and Child Node: A node, which is divided into sub-nodes is called a parent node of sub-nodes whereas sub-nodes are the child of a parent node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "from sklearn.tree import export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = export_graphviz(tree_clf, out_file= None, feature_names=iris.feature_names[2:], class_names=iris.target_names, rounded=True, filled=True\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'image.pdf'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = graphviz.Source(dot_data)\n",
    "graph.render(\"image\",view=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm that is used in Decision Tree to decide the best split\n",
    "* gini impurity\n",
    "* chi-square\n",
    "* information gain\n",
    "* reduction in variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of Gini Impurity\n",
    "\n",
    "* Node splits is decided by Gini impurity\n",
    "         gini impurity = 1 - gini\n",
    "    \n",
    "* Lower the gini impurity, higher the homogeneity of the nodes\n",
    "* Works only with categorical targets\n",
    "* Only perform binary splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to calculate gini impurity for a split\n",
    "\n",
    "* Calculate Gini Impurity for subnodes:\n",
    "    Gini Impurity = 1 - gini\n",
    "    \n",
    "* Gini = Sum of squares of probabilities for each class/category\n",
    "        gini = p1^2+p2^2+p3^2+...+pn^2\n",
    "\n",
    "* To calculate the gini impurity for a split, take the weighted gini impurity of both subnodes of that split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-square\n",
    "* statistical significance difference between the child nodes and the parent node\n",
    "* It is measured as the sum of squared standardized differences between actual and expected frequencies of the target variable for each node\n",
    "    chi-square sqrt[(actual-expected)^2/expected]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### properties of Chi-Square\n",
    "* Works only with categorical target variables\n",
    "* Higher Chi-Square value, higher homogeneity of the nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### information Gain\n",
    "Information gain is used when a node requires more information i.e the class is more heterogenous compared to other nodes.\n",
    "\n",
    "information gain = 1 - entropy\n",
    "\n",
    "entropy = -p1*log2p1-p2*log2p2-...-pn*log2pn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A set’s entropy is zero when it contains instances of only one class.\n",
    "After splitting, Information Gain is more than that of the parent node. This means that Child node is more homogenous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of Variance\n",
    "* Used when target is continuous\n",
    "* Split with lower variance is selected\n",
    "* The lesser the variance the better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the performance of decision tree\n",
    "\n",
    "* Minimum samples for a node\n",
    "    * Higher value control overfitting\n",
    "    * Too high value can lead to underfitting\n",
    "    \n",
    "* Minimum samples for a terminal node\n",
    "    * Higher value controls overfitting\n",
    "    * Too high value can lead to underfitting\n",
    "\n",
    "* Maximum depth of the tree\n",
    "    * Higher depth can lead to overfitting\n",
    "    * Lower depth can lead to underfitting\n",
    "\n",
    "* Maximum number of Terminal nodes\n",
    "    * prevent overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification and Regression Tree(CART) Algorithm\n",
    "* Greedy algorithm\n",
    "* NP Complete problem and requires exponential time complexity\n",
    "\n",
    "> Making prediction using decision tree requires traversing the Decision Tree and it\n",
    "requires going through roughly **O(log2(m))** nodes. That is also the prediction complexity. \n",
    "\n",
    "> However, the training algorithm compares all features (or less if max_features is set)\n",
    "on all samples at each node. This results in a training complexity of **O(n × m log(m))**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*P is the set of problems that can be solved in polynomial time. NP is the set of problems whose solutions can\n",
    "be verified in polynomial time. An NP-Hard problem is a problem to which any NP problem can be reduced\n",
    "in polynomial time. An NP-Complete problem is both NP and NP-Hard. A major open mathematical ques‐\n",
    "tion is whether or not P = NP. If P ≠ NP (which seems likely), then no polynomial algorithm will ever be\n",
    "found for any NP-Complete problem (except perhaps on a quantum computer).\n",
    "log2 is the binary logarithm. It is equal to log2\n",
    "(m) = log(m) / log(2).*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> So should you use Gini impurity or entropy? The truth is, most of the time it does not\n",
    "make a big difference: they lead to similar trees. Gini impurity is slightly faster to\n",
    "compute, so it is a good default. However, when they differ, Gini impurity tends to\n",
    "isolate the most frequent class in its own branch of the tree, while entropy tends to\n",
    "produce slightly more balanced trees.\n",
    "\n",
    "> To avoid overfitting the training data, you need to restrict the Decision Tree’s freedom\n",
    "during training. This is called regularization. The regularization\n",
    "hyperparameters depend on the algorithm used, but generally you can at least restrict\n",
    "the maximum depth of the Decision Tree. In Scikit-Learn, this is controlled by the\n",
    "max_depth hyperparameter (the default value is None, which means unlimited).\n",
    "Reducing max_depth will regularize the model and thus reduce the risk of overfitting.\n",
    "The DecisionTreeClassifier class has a few other parameters that similarly restrict\n",
    "the shape of the Decision Tree: min_samples_split (the minimum number of sam‐ples a node must have before it can be split), min_samples_leaf (the minimum num‐\n",
    "ber of samples a leaf node must have), min_weight_fraction_leaf (same as\n",
    "min_samples_leaf but expressed as a fraction of the total number of weighted\n",
    "instances), max_leaf_nodes (maximum number of leaf nodes), and max_features\n",
    "(maximum number of features that are evaluated for splitting at each node). Increas‐\n",
    "ing min_* hyperparameters or reducing max_* hyperparameters will regularize the\n",
    "model.\n",
    "\n",
    "\n",
    "\n",
    "> Other algorithms work by first training the Decision Tree without\n",
    "restrictions, then pruning (deleting) unnecessary nodes. A node\n",
    "whose children are all leaf nodes is considered unnecessary if the\n",
    "purity improvement it provides is not statistically significant. Stan‐\n",
    "dard statistical tests, such as the χ2\n",
    "test, are used to estimate the\n",
    "probability that the improvement is purely the result of chance\n",
    "(which is called the null hypothesis). If this probability, called the pvalue, is higher than a given threshold (typically 5%, controlled by\n",
    "a hyperparameter), then the node is considered unnecessary and its\n",
    "children are deleted. The pruning continues until all unnecessary\n",
    "nodes have been pruned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitation of Decision Tree\n",
    "\n",
    "* Very sensitive to small variations in the training data. In scikit learn, the model keeps changing except random state hyperparameter is set\n",
    "* Decision tree loves orthogonal decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
